{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nobackup1b/users/thiberio/clusterEvo/new_tests/archaea\n"
     ]
    }
   ],
   "source": [
    "%run ~/work/jupyter_notebooks/gene\\ family\\ distances/correlate_evolution.ipynb\n",
    "\n",
    "%cd ~/work/clusterEvo/new_tests/archaea/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import braycurtis\n",
    "from IPython.display        import HTML, display, clear_output\n",
    "from Bio                    import AlignIO, SeqIO, Align, Alphabet\n",
    "from copy                   import deepcopy\n",
    "\n",
    "import igraph     as ig\n",
    "import numpy      as np\n",
    "import seaborn    as sns\n",
    "import pandas     as pd\n",
    "import colorlover as cl\n",
    "\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import random\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import ete3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbi         = ete3.NCBITaxa()\n",
    "aln_alphabet = Alphabet.Gapped(Alphabet.IUPAC.ambiguous_dna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cd:\n",
    "    \"\"\"\n",
    "    Context manager for changing the current working directory\n",
    "    \"\"\"\n",
    "    def __init__(self, newPath):\n",
    "        self.newPath = os.path.expanduser(newPath)\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.savedPath = os.getcwd()\n",
    "        os.chdir(self.newPath)\n",
    "\n",
    "    def __exit__(self, etype, value, traceback):\n",
    "        os.chdir(self.savedPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 35\n",
      "1 15\n",
      "2 47\n",
      "3 111\n",
      "4 62\n",
      "5 28\n",
      "8 18\n",
      "15 15\n"
     ]
    }
   ],
   "source": [
    "graph                 = ig.Graph.Read_Picklez('coevolving_graph.pkl')\n",
    "minimum_size_clusters = []\n",
    "for clst_num in set(graph.vs['louvain']):\n",
    "    clst_nodes = graph.vs.select(louvain     =clst_num, \n",
    "#                                  trusted_clst=True\n",
    "                                )\n",
    "    if len(clst_nodes) < 10:# or clst_num < 0:\n",
    "        continue\n",
    "    \n",
    "#     if np.median(clst_nodes['num_taxa']) < 30:\n",
    "#         continue\n",
    "    \n",
    "    print(clst_num, len(clst_nodes))\n",
    "    minimum_size_clusters.append(clst_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 111), (4, 62), (2, 47), (0, 35), (5, 28), (8, 18), (1, 15), (15, 15)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(graph.vs.select(\n",
    "    louvain_in=minimum_size_clusters\n",
    ")['louvain']).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrices = []\n",
    "group_names   = []\n",
    "\n",
    "for clst_num in minimum_size_clusters:\n",
    "    clst_nodes = graph.vs.select(louvain     =clst_num,\n",
    "#                                  trusted_clst=True\n",
    "                                )\n",
    "    \n",
    "    for group_num in clst_nodes['name']:\n",
    "        try:\n",
    "            tmp_matrix = pd.read_csv(f'matrices/{group_num}.mldist', \n",
    "                                      delim_whitespace = True, \n",
    "                                      skiprows         = 1, \n",
    "                                      header           = None,\n",
    "                                      index_col        = 0)\n",
    "        except FileNotFoundError:\n",
    "            print(f'*Missing gene family {group_num}')\n",
    "            continue\n",
    "\n",
    "        convert_table = {}\n",
    "        for seq_name in re.findall('>(\\S+)', \n",
    "                                   open(f'minimum_size_groups/{group_num}.faa').read()):\n",
    "            seq_name = seq_name.split('|')\n",
    "            convert_table['_'.join(seq_name)] = f'{seq_name[1]}|{seq_name[0]}'\n",
    "\n",
    "        tmp_matrix.rename(index  =convert_table, \n",
    "                          inplace=True)\n",
    "        tmp_matrix.columns = tmp_matrix.index\n",
    "\n",
    "        dist_matrices.append(tmp_matrix.copy())\n",
    "        group_names.append(  group_num)\n",
    "    \n",
    "table = dict(zip(group_names, dist_matrices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxa(group_name, matrix):\n",
    "    taxa = [taxon.split('|')[0] for taxon in matrix.index]\n",
    "    return( group_name, taxa )\n",
    "\n",
    "group_taxa = pd.DataFrame( columns=['group_num', 'taxa'],\n",
    "                           data   =[get_taxa(group, table[group]) \n",
    "                                    for group in table.keys()] )\n",
    "group_taxa.set_index('group_num', inplace=True)\n",
    "\n",
    "all_genomes = set().union(*group_taxa.taxa.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clst_num in minimum_size_clusters:\n",
    "# for clst_num in [4]:\n",
    "\n",
    "    print(clst_num)\n",
    "\n",
    "    if not os.path.isfile(f'refined_cluster_phylogenies/cluster_{clst_num}_matching_copies.parquet'):\n",
    "        clst_subgraph = graph.vs.select(louvain     =clst_num,\n",
    "                                        trusted_clst=True).subgraph()\n",
    "        \n",
    "        if np.median(clst_subgraph.vs['num_taxa']) < 30:\n",
    "            continue\n",
    "\n",
    "        clst_subgraph.vs.select(clst_degree=0).delete()\n",
    "\n",
    "        matching_copies = pd.DataFrame(columns=clst_subgraph.vs['name'],\n",
    "                                       index  =all_genomes)\n",
    "        matching_copies = matching_copies.applymap(lambda x: [])\n",
    "\n",
    "        iteration_size = clst_subgraph.ecount()\n",
    "        for count, edge in enumerate(clst_subgraph.es):\n",
    "            group1 = edge.source_vertex['name']\n",
    "            group2 = edge.target_vertex['name']\n",
    "\n",
    "            tmp_matrix1 = table[group1].copy()\n",
    "            tmp_matrix2 = table[group2].copy()\n",
    "\n",
    "            matrix1, taxa1, matrix2, taxa2 = balance_matrices(tmp_matrix1.copy(), \n",
    "                                                              tmp_matrix2.copy(), \n",
    "                                                              gene_sep   ='|')\n",
    "\n",
    "            for (index1, row1), (index2, row2) in zip(taxa1.iterrows(), taxa2.iterrows()):\n",
    "                matching_copies.loc[row1.genome, group1].append(row1.gene)\n",
    "                matching_copies.loc[row2.genome, group2].append(row2.gene)\n",
    "\n",
    "            print('\\rIteration '+str(round((count/iteration_size)*100, 2))+'%', end='')\n",
    "\n",
    "        count += 1\n",
    "        print('\\rIteration '+str(round((count/iteration_size)*100, 2))+'%', end='')\n",
    "\n",
    "        matching_copies.rename(columns=dict( zip(matching_copies.columns, \n",
    "                                                 matching_copies.columns.astype(str)) ),\n",
    "                               inplace=True)\n",
    "        matching_copies.to_parquet(f'refined_cluster_phylogenies/cluster_{clst_num}_matching_copies.parquet')\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clst_num in minimum_size_clusters:\n",
    "    clst_nodes = graph.vs.select(louvain=clst_num)\n",
    "            \n",
    "    print(clst_num)\n",
    "\n",
    "    if not os.path.isfile(f'cluster_phylogenies/cluster_{clst_num}_matching_copies.parquet'):\n",
    "        clst_subgraph = graph.vs.select(louvain=clst_num).subgraph()\n",
    "        clst_subgraph.vs.select(clst_degree=0).delete()\n",
    "\n",
    "        matching_copies = pd.DataFrame(columns=clst_subgraph.vs['name'],\n",
    "                                       index  =all_genomes)\n",
    "        matching_copies = matching_copies.applymap(lambda x: [])\n",
    "\n",
    "        iteration_size = clst_subgraph.ecount()\n",
    "        for count, edge in enumerate(clst_subgraph.es):\n",
    "            group1 = edge.source_vertex['name']\n",
    "            group2 = edge.target_vertex['name']\n",
    "\n",
    "            tmp_matrix1 = table[group1].copy()\n",
    "            tmp_matrix2 = table[group2].copy()\n",
    "\n",
    "            matrix1, taxa1, matrix2, taxa2 = balance_matrices(tmp_matrix1.copy(), \n",
    "                                                              tmp_matrix2.copy(), \n",
    "                                                              gene_sep   ='|')\n",
    "\n",
    "            for (index1, row1), (index2, row2) in zip(taxa1.iterrows(), taxa2.iterrows()):\n",
    "                matching_copies.loc[row1.genome, group1].append(row1.gene)\n",
    "                matching_copies.loc[row2.genome, group2].append(row2.gene)\n",
    "\n",
    "            print('\\rIteration '+str(round((count/iteration_size)*100, 2))+'%', end='')\n",
    "\n",
    "        count += 1\n",
    "        print('\\rIteration '+str(round((count/iteration_size)*100, 2))+'%', end='')\n",
    "\n",
    "        matching_copies.rename(columns=dict( zip(matching_copies.columns, \n",
    "                                                 matching_copies.columns.astype(str)) ),\n",
    "                               inplace=True)\n",
    "        matching_copies.to_parquet(f'cluster_phylogenies/cluster_{clst_num}_matching_copies.parquet')\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_copy_supports = []\n",
    "for clst_num in minimum_size_clusters:\n",
    "    \n",
    "    matching_copies = pd.read_parquet(f'cluster_phylogenies/cluster_{clst_num}_matching_copies.parquet')\n",
    "\n",
    "    to_drop = matching_copies.columns.intersection(graph.vs.select(single_copy=True)['name'])\n",
    "    matching_copies.drop(columns=to_drop, \n",
    "                         inplace=True)\n",
    "\n",
    "    matching_copy_freq = matching_copies.applymap(   lambda x: Counter(x))\n",
    "    best_copies        = matching_copy_freq.applymap(lambda x: x.most_common()[0][0] \n",
    "                                                               if x \n",
    "                                                               else np.nan)\n",
    "    tmp_support = matching_copy_freq.applymap(lambda cell: \n",
    "                                              max(cell.values()) / sum(cell.values()) if cell \n",
    "                                              else np.nan)\n",
    "    \n",
    "    best_copy_supports.extend( list(itertools.chain(*tmp_support.values)) )\n",
    "\n",
    "best_copy_supports = np.array(best_copy_supports)\n",
    "best_copy_supports = best_copy_supports[pd.notna(best_copy_supports)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(best_copy_supports, 3.5), np.median(best_copy_supports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_genomes_with_duplications = []\n",
    "number_of_genomes                   = []\n",
    "for node in graph.vs:\n",
    "    \n",
    "    group_num = node['name']\n",
    "    with open(f'minimum_size_groups/{group_num}.faa') as fasta_handle:\n",
    "        \n",
    "        group_genomes = re.findall('^>[^|]+?\\|(.*)$', fasta_handle.read(), re.M)\n",
    "        if len(group_genomes) == len(set(group_genomes)):\n",
    "            continue\n",
    "            \n",
    "        number_of_genomes.append(node['num_taxa'])\n",
    "        \n",
    "        duplicated_genomes_count = 0\n",
    "        for genome, copy_count in Counter(group_genomes).most_common():\n",
    "            if copy_count == 1:\n",
    "                break\n",
    "            duplicated_genomes_count += 1\n",
    "        \n",
    "        number_of_genomes_with_duplications.append( duplicated_genomes_count )\n",
    "    \n",
    "number_of_genomes_with_duplications = np.array(number_of_genomes_with_duplications)\n",
    "number_of_genomes                   = np.array(number_of_genomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "419-len(number_of_genomes_with_duplications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_genomes_with_duplications[(number_of_genomes_with_duplications <= 2) & (number_of_genomes >= 35)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_genomes_with_duplications[number_of_genomes >= 35].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_genomes_with_duplications[number_of_genomes_with_duplications <= 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(graph.vs.select(louvain    =4, \n",
    "                    single_copy=True,\n",
    "                    num_taxa_ge=35\n",
    "                   )\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(graph.vs.select(louvain    =4, \n",
    "                    single_copy=False,\n",
    "#                     num_taxa_ge=35\n",
    "                   )\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5), dpi=100)\n",
    "\n",
    "sns.kdeplot(\n",
    "    number_of_genomes_with_duplications, \n",
    "#     bw   =1, \n",
    "    shade=True, \n",
    "    ax   =ax);\n",
    "\n",
    "ax.set_title('# genomes containing multiple copies')\n",
    "ax.set_xlim(0)\n",
    "sns.despine(offset=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5), dpi=100)\n",
    "\n",
    "sns.kdeplot(\n",
    "    best_copy_supports, \n",
    "    bw   =0.02, \n",
    "    shade=True, \n",
    "    ax   =ax);\n",
    "\n",
    "ax.set_title('Within cluster confidence in most frequent paralog')\n",
    "sns.despine(offset=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fasta(column, target_folder):\n",
    "    header2seq = {}\n",
    "    with open(f'minimum_size_groups/{column[0]}.faa') as fasta:\n",
    "        for line in fasta:\n",
    "            if line.startswith('>'):\n",
    "                header              = line.strip('>\\n ')\n",
    "                header2seq[header]  = ''\n",
    "            else:\n",
    "                line                = line.strip(' \\n')\n",
    "                header2seq[header] += line.replace('-', '')\n",
    "    \n",
    "    with open(f'{target_folder}/{column[0]}.faa', 'w') as fasta:\n",
    "        for genome_acc, protein_id in column[1].items():\n",
    "            if pd.isna(protein_id):\n",
    "                continue\n",
    "\n",
    "            seq = header2seq[f'{protein_id}|{genome_acc}']\n",
    "            fasta.write(f'>{genome_acc}\\n{seq}\\n')\n",
    "\n",
    "    return(column[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_alignment(input_arg):\n",
    "    group_num, clst_num = input_arg\n",
    "    with open(f'refined_cluster_phylogenies/cluster_{clst_num}_alignments/{group_num}.aln', 'w') as aln_out:\n",
    "        subprocess.call(['mafft', \n",
    "                         '--auto',\n",
    "                         '--reorder', \n",
    "                         '--quiet', \n",
    "                         f'refined_cluster_phylogenies/cluster_{clst_num}_alignments/{group_num}.faa'], \n",
    "                        stdout=aln_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concate_genes(group_numbers, clst_num):\n",
    "\n",
    "    concat_genomes = set().union(*group_taxa.loc[group_numbers, 'taxa'].values)\n",
    "    \n",
    "    missing_genes = {} # just to keep track of the number of missing marker genes in each genome\n",
    "    concatenation = {}\n",
    "    for genome in concat_genomes:\n",
    "        missing_genes[genome]             = 0\n",
    "        concatenation[genome]             = Align.SeqRecord( Align.Seq('', aln_alphabet) )\n",
    "        concatenation[genome].name        = genome\n",
    "        concatenation[genome].id          = genome\n",
    "        concatenation[genome].description = genome\n",
    "    \n",
    "    total_genes      = 0.0 # keep track of the number of genes added to the concatenation\n",
    "    current_position = 1\n",
    "    aln_folder       = f'refined_cluster_phylogenies/cluster_{clst_num}_alignments'\n",
    "    partitions       = open(f'refined_cluster_phylogenies/cluster_{clst_num}', 'w')\n",
    "    for group_num in group_numbers:\n",
    "\n",
    "        tmp_aln      = AlignIO.read( f'{aln_folder}/{group_num}.aln', 'fasta' )\n",
    "        aln_length   = tmp_aln.get_alignment_length() # get the expected size of the alignment so you can compare if all have the same size\n",
    "        total_genes += aln_length\n",
    "\n",
    "        genomes_found = set()\n",
    "        for entry in tmp_aln:\n",
    "            # if this alignment has a different size from the rest, something is reaaaaaly wrong!\n",
    "            if len(entry) != aln_length:\n",
    "                print('\\t**Error, block \"%s\" has a different length than the rest of the MSA: %s' %(entry.name, aln))\n",
    "            \n",
    "            concatenation[entry.name] += deepcopy(entry.seq)\n",
    "            genomes_found.add(entry.name)\n",
    "\n",
    "        partition_name = f'cluster{group_num}'\n",
    "        partitions.write(f'LG, {partition_name} = {current_position}-{current_position+aln_length-1}\\n')\n",
    "        current_position += aln_length\n",
    "\n",
    "        #\n",
    "        # add gaps for those genomes missing this gene (same size as the expected alignment)\n",
    "        for genome in concat_genomes.difference(genomes_found):\n",
    "            concatenation[genome] += Align.Seq( '-' * aln_length, aln_alphabet )\n",
    "            missing_genes[genome] += aln_length\n",
    "    partitions.close()\n",
    "\n",
    "    AlignIO.write( Align.MultipleSeqAlignment( concatenation.values() ), \n",
    "                  f'refined_cluster_phylogenies/cluster_{clst_num}.aln', 'fasta' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clst_num in minimum_size_clusters:\n",
    "    print(clst_num)\n",
    "\n",
    "    matching_copies = pd.read_parquet(f'refined_cluster_phylogenies/cluster_{clst_num}_matching_copies.parquet')\n",
    "\n",
    "    matching_copy_freq = matching_copies.applymap(   lambda x: Counter(x))\n",
    "    best_copies        = matching_copy_freq.applymap(lambda x: x.most_common()[0][0] \n",
    "                                                               if x \n",
    "                                                               else np.nan)\n",
    "    \n",
    "    if not os.path.isdir(f'refined_cluster_phylogenies/cluster_{clst_num}_alignments'):\n",
    "        os.mkdir(f'refined_cluster_phylogenies/cluster_{clst_num}_alignments')\n",
    "    \n",
    "    aln_func_input = []\n",
    "    for column in best_copies.iteritems():\n",
    "        generate_fasta(column, \n",
    "                       target_folder=f'refined_cluster_phylogenies/cluster_{clst_num}_alignments')\n",
    "        aln_func_input.append( (column[0], clst_num) )\n",
    "    \n",
    "    pool     = multiprocessing.Pool( processes=15 )\n",
    "    ran_alns = pool.map(run_alignment, aln_func_input)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "#     clst_nodes    = graph.vs.select(louvain=clst_num)\n",
    "    clst_subgraph = graph.vs.select(louvain     =clst_num,\n",
    "                                    trusted_clst=True).subgraph()\n",
    "    clst_subgraph.vs['degree'] = clst_subgraph.degree()\n",
    "    clst_subgraph.vs.select(degree=0).delete()\n",
    "\n",
    "    concate_genes(clst_subgraph.vs['name'], clst_num)\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = 'assembly_accession bioproject biosample wgs_master refseq_category taxid species_taxid \\\n",
    "          organism_name infraspecific_name isolate version_status assembly_level release_type \\\n",
    "          genome_rep seq_rel_date asm_name submitter gbrs_paired_asm paired_asm_comp ftp_path \\\n",
    "          excluded_from_refseq relation_to_type_material'.split()\n",
    "\n",
    "genbank_summary = pd.read_csv('~/work/assembly_summary_genbank.txt', \n",
    "                              sep      ='\\t', \n",
    "                              index_col=0, \n",
    "                              header   =None, \n",
    "                              names    =header, \n",
    "                              comment  ='#')\n",
    "genbank_summary = genbank_summary.reindex(index=all_genomes).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clst_num in minimum_size_clusters:\n",
    "    with open(f'cluster_phylogenies/cluster_{clst_num}.treefile') as tree_handle:\n",
    "        newick  = tree_handle.read()\n",
    "        \n",
    "    for index, row in genbank_summary.iterrows():\n",
    "        newick = newick.replace(index, \n",
    "                                str(row.taxid))\n",
    "    \n",
    "    with open(f'cluster_phylogenies/cluster_{clst_num}.taxid.treefile', 'w') as tree_handle:\n",
    "        tree_handle.write(newick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clst_num in [2, 3, 4, 5]:\n",
    "    with open(f'cluster_phylogenies/cluster_{clst_num}-C60.treefile') as tree_handle:\n",
    "        newick  = tree_handle.read()\n",
    "        \n",
    "    for index, row in genbank_summary.iterrows():\n",
    "        newick = newick.replace(index, \n",
    "                                str(row.taxid))\n",
    "    \n",
    "    with open(f'cluster_phylogenies/cluster_{clst_num}-C60.taxid.treefile', 'w') as tree_handle:\n",
    "        tree_handle.write(newick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'single_copy_phylogenies/core_genes_from_cluster_all-C60.treefile') as tree_handle:\n",
    "    newick  = tree_handle.read()\n",
    "\n",
    "for index, row in genbank_summary.iterrows():\n",
    "    newick = newick.replace(index, \n",
    "                            str(row.taxid))\n",
    "\n",
    "with open(f'single_copy_phylogenies/core_genes_from_cluster_all-C60.taxid.treefile', 'w') as tree_handle:\n",
    "    tree_handle.write(newick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for clst_num in minimum_size_clusters:\n",
    "for clst_num in [3, 4, 5]:\n",
    "    with open(f'refined_cluster_phylogenies/cluster_{clst_num}-C20.treefile') as tree_handle:\n",
    "        newick  = tree_handle.read()\n",
    "        \n",
    "    for index, row in genbank_summary.iterrows():\n",
    "        newick = newick.replace(index, \n",
    "                                str(row.taxid))\n",
    "    \n",
    "    with open(f'refined_cluster_phylogenies/cluster_{clst_num}-C20.taxid.treefile', 'w') as tree_handle:\n",
    "        tree_handle.write(newick)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
